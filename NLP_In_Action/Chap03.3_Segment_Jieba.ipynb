{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"中文分词是文本处理不可或缺的一步！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式： ['中文', '分词', '是', '文本', '文本处理', '本处', '处理', '不可', '不可或缺', '或缺', '的', '一步', '', '']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"全模式\"\"\" # 会将全部分词可能都输出\n",
    "seg_list = jieba.lcut(sent, cut_all=True)  ## lcut 返回的是列表， cut返回的是generator\n",
    "print(\"全模式：\", seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精确模式： ['中文', '分词', '是', '文本处理', '不可或缺', '的', '一步', '！']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"精确模式（默认）\"\"\"\n",
    "seg_list = jieba.lcut(sent, cut_all=False)\n",
    "print(\"精确模式：\", seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索引擎模式： ['中文', '分词', '是', '文本', '本处', '处理', '文本处理', '不可', '或缺', '不可或缺', '的', '一步', '！']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"搜索引擎模式\"\"\"  # 在精确模式基础上对长词再切分\n",
    "seg_list = jieba.lcut_for_search(sent, HMM=True)\n",
    "print(\"搜索引擎模式：\", seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高频词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C000008',\n",
       " 'C000010',\n",
       " 'C000013',\n",
       " 'C000014',\n",
       " 'C000016',\n",
       " 'C000020',\n",
       " 'C000022',\n",
       " 'C000023',\n",
       " 'C000024']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_from_doc(file_path):\n",
    "    \"\"\"\n",
    "    文件内容读取函数\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "    with open(file_path, mode=\"r\", encoding=\"gbk\", errors=\"ignore\") as f:\n",
    "        for l in f:\n",
    "            l = l.strip()\n",
    "            content += l\n",
    "    return content\n",
    "\n",
    "def get_TF(words):\n",
    "    \"\"\"\n",
    "    词频统计函数\n",
    "    words: 包含词的列表或生成器\n",
    "    \"\"\"\n",
    "    tf_dic = {}\n",
    "    for word in words:\n",
    "        try:\n",
    "            tf_dic[word] += 1\n",
    "        except KeyError:\n",
    "            tf_dic[word] = 1\n",
    "    return tf_dic\n",
    "\n",
    "def get_topK_words(tf_dic, topK=10, stop_words=None):\n",
    "    \"\"\"\n",
    "    去除stop_words统计，根据词频统计，返回词频最高的 topK个词\n",
    "    \"\"\"\n",
    "    if stop_words is not None:\n",
    "        tf_dic = {k: v for k, v in tf_dic.items() if k not in stop_words}\n",
    "    return sorted(tf_dic.items(), key=lambda x: x[1], reverse=True)[:topK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本文件： F:/for learn/Python/NLP_in_Action/chapter-3/data/news/C000013\\1855.txt\n",
      "样本之一： 《美国新闻与世界报道》杂志邀请数位饮食、理财和医学专家，总结出50个在新的一年提高生活质量的建议，与健康相关的介绍如下：每天补充一点欧米伽3欧米伽3是一种不饱和脂肪酸，鱼类、芥菜籽油和胡桃等食物中都富含这一营养成分。常吃含欧米伽3的食物，能预防心脏病。&nbsp; 收听、收看医学新闻美国曾在2005年夏季进行过一项民意调查，结果显示，美国人极其依赖报纸、广播和电视，来获取健康方面的新闻。因此，电台和电视台应加长播报健康新闻的时间，播报一条健康新闻不能少于90秒。&nbsp; 挑选正确的止痛药2005年以来，关于止痛药的负面信息不断地出现在媒体上，美国食品和药品监督管理局对止痛药采取了监管措施。这也给人们提了个醒：购买止痛药必须要谨慎、按医嘱，严格按照推荐剂量。给自己买一部振动健身器买一部振动健身器放在家中，运动起来不累，而且不占地方。和孩子一起玩电脑游戏父母对电脑游戏应有正确的态度，甚至应加入到孩子们的行列，毕竟玩游戏可以提高眼、手的协调能力，对大脑也很有好处。冥想，大脑休息的好方法研究证明，冥想可以缓解身体紧张，治疗心脏病、关节炎等疾病。每天只要花上十多分钟，还能大大降低心血管病发生率。享受一把泰式按摩泰式按摩是结合瑜伽、按摩、针灸为一体的保健方法，有助于缓减身体某些部位疼痛。按摩前，一定要向按摩师介绍自己的病史，这样可以做到有的放矢，也能避免意外损伤。杀菌肥皂少用研究发现，灭菌肥皂和普通肥皂在杀灭细菌、防感染的效果一样，但长期使用灭菌肥皂会产生抗药性。少吃人造黄油人造黄油含有反式脂肪酸，会提高人体坏胆固醇水平，长期食用会增加患心脏病风险。不再补充维生素E19项研究发现，大剂量服用维生素E会增加中风几率，还会减少体内可溶解脂肪的抗氧化剂。\n",
      "样本分词结果: 《/ 美国/ 新闻/ 与/ 世界/ 报道/ 》/ 杂志/ 邀请/ 数位/ 饮食/ 、/ 理财/ 和/ 医学专家/ ，/ 总结/ 出/ 50/ 个/ 在/ 新/ 的/ 一年/ 提高/ 生活/ 质量/ 的/ 建议/ ，/ 与/ 健康/ 相关/ 的/ 介绍/ 如下/ ：/ 每天/ 补充/ 一点/ 欧米/ 伽/ 3/ 欧米/ 伽/ 3/ 是/ 一种/ 不饱和/ 脂肪酸/ ，/ 鱼类/ 、/ 芥/ 菜籽油/ 和/ 胡桃/ 等/ 食物/ 中/ 都/ 富含/ 这一/ 营养成分/ 。/ 常吃含/ 欧米/ 伽/ 3/ 的/ 食物/ ，/ 能/ 预防/ 心脏病/ 。/ &/ nbsp/ ;/  / 收听/ 、/ 收看/ 医学/ 新闻/ 美国/ 曾/ 在/ 2005/ 年/ 夏季/ 进行/ 过/ 一项/ 民意调查/ ，/ 结果显示/ ，/ 美国/ 人/ 极其/ 依赖/ 报纸/ 、/ 广播/ 和/ 电视/ ，/ 来/ 获取/ 健康/ 方面/ 的/ 新闻/ 。/ 因此/ ，/ 电台/ 和/ 电视台/ 应/ 加长/ 播报/ 健康/ 新闻/ 的/ 时间/ ，/ 播报/ 一条/ 健康/ 新闻/ 不能/ 少于/ 90/ 秒/ 。/ &/ nbsp/ ;/  / 挑选/ 正确/ 的/ 止痛药/ 2005/ 年/ 以来/ ，/ 关于/ 止痛药/ 的/ 负面/ 信息/ 不断/ 地/ 出现/ 在/ 媒体/ 上/ ，/ 美国/ 食品/ 和/ 药品监督管理局/ 对/ 止痛药/ 采取/ 了/ 监管/ 措施/ 。/ 这/ 也/ 给/ 人们/ 提了/ 个/ 醒/ ：/ 购买/ 止痛药/ 必须/ 要/ 谨慎/ 、/ 按/ 医嘱/ ，/ 严格/ 按照/ 推荐/ 剂量/ 。/ 给/ 自己/ 买/ 一部/ 振动/ 健身器/ 买/ 一部/ 振动/ 健身器/ 放在/ 家中/ ，/ 运动/ 起来/ 不/ 累/ ，/ 而且/ 不/ 占/ 地方/ 。/ 和/ 孩子/ 一起/ 玩/ 电脑游戏/ 父母/ 对/ 电脑游戏/ 应有/ 正确/ 的/ 态度/ ，/ 甚至/ 应/ 加入/ 到/ 孩子/ 们/ 的/ 行列/ ，/ 毕竟/ 玩游戏/ 可以/ 提高/ 眼/ 、/ 手/ 的/ 协调/ 能力/ ，/ 对/ 大脑/ 也/ 很/ 有/ 好处/ 。/ 冥想/ ，/ 大脑/ 休息/ 的/ 好/ 方法/ 研究/ 证明/ ，/ 冥想/ 可以/ 缓解/ 身体/ 紧张/ ，/ 治疗/ 心脏病/ 、/ 关节炎/ 等/ 疾病/ 。/ 每天/ 只要/ 花上/ 十多分钟/ ，/ 还/ 能/ 大大降低/ 心血管病/ 发生率/ 。/ 享受/ 一把/ 泰式/ 按摩/ 泰式/ 按摩/ 是/ 结合/ 瑜伽/ 、/ 按摩/ 、/ 针灸/ 为/ 一体/ 的/ 保健/ 方法/ ，/ 有助于/ 缓减/ 身体/ 某些/ 部位/ 疼痛/ 。/ 按摩/ 前/ ，/ 一定/ 要/ 向/ 按摩师/ 介绍/ 自己/ 的/ 病史/ ，/ 这样/ 可以/ 做到/ 有的放矢/ ，/ 也/ 能/ 避免/ 意外/ 损伤/ 。/ 杀菌/ 肥皂/ 少/ 用/ 研究/ 发现/ ，/ 灭菌/ 肥皂/ 和/ 普通/ 肥皂/ 在/ 杀灭/ 细菌/ 、/ 防/ 感染/ 的/ 效果/ 一样/ ，/ 但/ 长期/ 使用/ 灭菌/ 肥皂/ 会/ 产生/ 抗药性/ 。/ 少/ 吃/ 人造黄油/ 人造黄油/ 含有/ 反式/ 脂肪酸/ ，/ 会/ 提高/ 人体/ 坏/ 胆固醇/ 水平/ ，/ 长期/ 食用/ 会/ 增加/ 患/ 心脏病/ 风险/ 。/ 不再/ 补充/ 维生素/ E19/ 项/ 研究/ 发现/ ，/ 大/ 剂量/ 服用/ 维生素/ E/ 会/ 增加/ 中风/ 几率/ ，/ 还会/ 减少/ 体内/ 可溶解/ 脂肪/ 的/ 抗氧化剂/ 。\n",
      "样本topK分词： [('新闻', 5), ('美国', 4), ('健康', 4), ('止痛药', 4), ('按摩', 4), ('肥皂', 4), ('会', 4), ('提高', 3), ('欧米', 3), ('伽', 3)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random \n",
    "import jieba\n",
    "\n",
    "path = \"F:/for learn/Python/NLP_in_Action/chapter-3/data/\"\n",
    "stop_words_path = path + \"stop_words.utf8\"\n",
    "content_path = path + \"news/\"\n",
    "\n",
    "with open(stop_words_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "    stop_words = [l.strip() for l in f]\n",
    "\n",
    "files = glob.glob(content_path + \"C000013/*.txt\")\n",
    "## 随机选择一个文本作为样本\n",
    "rand_ind = random.randint(0, len(files))\n",
    "sample_corpus = get_content_from_doc(files[rand_ind])\n",
    "\n",
    "split_words = jieba.lcut(sample_corpus)\n",
    "words_tf = get_TF(split_words)\n",
    "topK_words = get_topK_words(words_tf, stop_words=stop_words)\n",
    "\n",
    "print(\"样本文件：\", files[rand_ind])\n",
    "print(\"样本之一：\", sample_corpus)\n",
    "print(\"样本分词结果:\", \"/ \".join(split_words))\n",
    "print(\"样本topK分词：\", str(topK_words))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
