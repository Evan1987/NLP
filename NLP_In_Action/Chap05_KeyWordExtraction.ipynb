{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "import functools\n",
    "\n",
    "path = \"F:/for learn/Python/NLP_in_Action/chapter-5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list(file_path):\n",
    "    \"\"\"\n",
    "    加载停用词\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        l = [s.replace(\"\\n\", \"\") for s in f.readlines()]\n",
    "    return l\n",
    "\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    \"\"\"\n",
    "    分词方法，调用jieba接口\n",
    "    input:\n",
    "    - sentence: 句子, string\n",
    "    - pos: 是否采用词性标注分词, boolean\n",
    "    \"\"\"\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        # 采用词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "        \n",
    "def word_filter(seg_list, stopword_list):\n",
    "    \"\"\"\n",
    "    去除干扰词\n",
    "    input:\n",
    "    - seg_list: 分词列表，generator[String]或generator[(String, String)]\n",
    "    - stopword_list: 停用词列表， List[String]\n",
    "    \"\"\"\n",
    "    filter_list = []\n",
    "    # 由于分词列表可能是由词性标注生成的，因此接下来处理时需要判断\n",
    "    pos = True if len(seg_list[0]) > 1 else False\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "        else:\n",
    "            word, flag = seg\n",
    "            # 只保留名词\n",
    "            if not flag.startswith(\"n\"):\n",
    "                continue\n",
    "        # 过滤掉长度小于2的词 和 停用词\n",
    "        if len(word) < 2 | word in stopword_list:\n",
    "            continue\n",
    "        \n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def load_data(corpus_path, pos=False):\n",
    "    \"\"\"\n",
    "    加载语料数据\n",
    "    input: \n",
    "    - corpus_path: 语料地址\n",
    "    - pos: 决定是否用词性分词\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    with open(corpus_path, \"r\", encoding=\"UTf-8\") as f:\n",
    "        for line in f:\n",
    "            content = line.strip()\n",
    "            seg_list = seg_to_list(content, pos)\n",
    "            filter_list = word_filter(seg_list, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
