{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "path = \"F:/for learn/Python/NLP_in_Action/chapter-5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():\n",
    "    \"\"\"\n",
    "    加载停用词\n",
    "    \"\"\"\n",
    "    file_path = path + \"stopword.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        l = [s.replace(\"\\n\", \"\") for s in f.readlines()]\n",
    "        f.close()\n",
    "    return l\n",
    "\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    \"\"\"\n",
    "    分词方法，调用jieba接口\n",
    "    input:\n",
    "    - sentence: 句子, string\n",
    "    - pos: 是否采用词性标注分词, boolean\n",
    "    \"\"\"\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        # 采用词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "        \n",
    "def word_filter(seg_list, stopword_list):\n",
    "    \"\"\"\n",
    "    去除干扰词\n",
    "    input:\n",
    "    - seg_list: 分词列表，generator[String]或generator[(String, String)]\n",
    "    - stopword_list: 停用词列表， List[String]\n",
    "    \"\"\"\n",
    "    filter_list = []\n",
    "    # 由于分词列表可能是由词性标注生成的，因此接下来处理时需要判断\n",
    "    pos = True if len(seg_list[0]) > 1 else False\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "        else:\n",
    "            word, flag = seg\n",
    "            # 只保留名词\n",
    "            if not flag.startswith(\"n\"):\n",
    "                continue\n",
    "        # 过滤掉长度小于2的词 和 停用词\n",
    "        if len(word) < 2 | word in stopword_list:\n",
    "            continue\n",
    "        \n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def load_data(corpus_path, pos=False):\n",
    "    \"\"\"\n",
    "    加载语料数据\n",
    "    input: \n",
    "    - corpus_path: 语料地址\n",
    "    - pos: 决定是否用词性分词\n",
    "    :return: 语料二重列表， 第一重为语料，第二重为该语料的词（已去除停用词），如果pos为True，则为（词、词性）tuple\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    stopword_list = get_stopword_list()\n",
    "    with open(corpus_path, \"r\", encoding=\"UTf-8\") as f:\n",
    "        for line in f:\n",
    "            content = line.strip()\n",
    "            seg_list = seg_to_list(content, pos)\n",
    "            filter_list = word_filter(seg_list, stopword_list)\n",
    "            doc_list.append(filter_list)\n",
    "        f.close()\n",
    "    return doc_list  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, keyword_num):\n",
    "        \"\"\"\n",
    "        :param keyword_num: 关键词数量\n",
    "        \"\"\"\n",
    "        self.keyword_num = keyword_num\n",
    "    def fit(self, doc_list):\n",
    "        \"\"\"\n",
    "        根据训练语料，得到词的idf值\n",
    "        :param doc_list: 训练语料\n",
    "        :return: 包含语料各个词的idf字典，以及未登录词的默认idf值\n",
    "        \"\"\"\n",
    "        idf_dic = defaultdict(int)\n",
    "        tt_count = 0\n",
    "        for word_list in doc_list:\n",
    "            for word in set(word_list):\n",
    "                idf_dic[word] += 1\n",
    "            tt_count += 1\n",
    "\n",
    "        self.idf_dic = {word: math.log(tt_count / (num + 1.0)) for word, num in idf_dic.items()}\n",
    "        self.default_idf = math.log(tt_count / 1.0)\n",
    "    \n",
    "    def transform(self, word_list):\n",
    "        \"\"\"\n",
    "        对目标词列表进行tf-idf转换\n",
    "        :param word_list: 目标词列表\n",
    "        \"\"\"\n",
    "        # 统计词频\n",
    "        tf_dic = defaultdict(int)\n",
    "        for word in word_list:\n",
    "            tf_dic[word] += 1\n",
    "        \n",
    "        self.tfidf_dic = {word: tf * self.idf_dic.get(word, self.default_idf) for word, tf in tf_dic.items()}\n",
    "        keywords = sorted(list(self.tfidf_dic.items()), key=lambda tup: tup[1], reverse=True)[:self.keyword_num]\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel:\n",
    "    def __init__(self, keyword_num, num_topics=4):\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "    def fit(self, doc_list):\n",
    "        self.tfidf = TfidfVectorizer(min_df=0)\n",
    "        vec = self.tfidf.fit_transform(doc_list)\n",
    "        self.model = LatentDirichletAllocation(n_topics=self.num_topics, random_state=0)\n",
    "        self.model.fit(vec)\n",
    "    def transform(self, word_list):\n",
    "        content = \" \".join(word_list)\n",
    "        seq_vec = self.tfidf.transform([content])  # sparsed\n",
    "        seq_topic_vec = self.model.transform(seq_vec)  # 1 * num_topis 语句的主题分布\n",
    "        word_mat = self.tfidf.transform([word])\n",
    "        word_topic_mat = self.model.transform(word_mat)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mat = tfidf.transform(docs[0])\n",
    "word_topic_mat = model.transform(word_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76664136, 0.07755041, 0.07790411, 0.07790411]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_vec = tfidf.transform([content])\n",
    "seq_topic_vec = model.transform(seq_vec)\n",
    "seq_topic_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      ],\n",
       "       [0.50631757],\n",
       "       [0.25      ],\n",
       "       [0.25      ],\n",
       "       [0.50631757],\n",
       "       [0.50631757],\n",
       "       [0.50631757],\n",
       "       [0.50631757]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(word_topic_mat, seq_topic_vec.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \" \".join(docs[0])\n",
    "vec = tfidf.transform([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=0)\n",
    "vec = tfidf.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=4, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LatentDirichletAllocation(n_topics=4, random_state=0)\n",
    "model.fit(vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
