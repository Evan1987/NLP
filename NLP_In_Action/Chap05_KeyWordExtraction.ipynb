{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "import warnings\n",
    "from jieba import analyse\n",
    "from _utils import u_constant\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "path = u_constant.PATH_ROOT + \"for learn/Python/NLP_in_Action/chapter-5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():\n",
    "    \"\"\"\n",
    "    加载停用词\n",
    "    \"\"\"\n",
    "    file_path = path + \"stopword.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        l = [s.replace(\"\\n\", \"\") for s in f.readlines()]\n",
    "        f.close()\n",
    "    return l\n",
    "\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    \"\"\"\n",
    "    分词方法，调用jieba接口\n",
    "    input:\n",
    "    - sentence: 句子, string\n",
    "    - pos: 是否采用词性标注分词, boolean\n",
    "    \"\"\"\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.lcut(sentence)\n",
    "    else:\n",
    "        # 采用词性标注的分词方法\n",
    "        seg_list = psg.lcut(sentence)\n",
    "    return seg_list\n",
    "        \n",
    "def word_filter(seg_list, stopword_list):\n",
    "    \"\"\"\n",
    "    去除干扰词\n",
    "    input:\n",
    "    - seg_list: 分词列表，generator[String]或generator[(String, String)]\n",
    "    - stopword_list: 停用词列表， List[String]\n",
    "    \"\"\"\n",
    "    filter_list = []\n",
    "    # 由于分词列表可能是由词性标注生成的，因此接下来处理时需要判断\n",
    "    pos = isinstance(seg_list[0], jieba.posseg.pair)\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "        else:\n",
    "            word, flag = seg\n",
    "            # 只保留名词\n",
    "            if not flag.startswith(\"n\"):\n",
    "                continue\n",
    "        # 过滤掉长度小于2的词 和 停用词\n",
    "        if (len(word) < 2) | (word in stopword_list):\n",
    "            continue\n",
    "        \n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def load_data(corpus_path, pos=False):\n",
    "    \"\"\"\n",
    "    加载语料数据\n",
    "    input: \n",
    "    - corpus_path: 语料地址\n",
    "    - pos: 决定是否用词性分词\n",
    "    :return: 语料二重列表， 第一重为语料，第二重为该语料的词（已去除停用词），如果pos为True，则为（词、词性）tuple\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    stopword_list = get_stopword_list()\n",
    "    with open(corpus_path, \"r\", encoding=\"UTf-8\") as f:\n",
    "        for line in f:\n",
    "            content = line.strip()\n",
    "            seg_list = seg_to_list(content, pos)\n",
    "            filter_list = word_filter(seg_list, stopword_list)\n",
    "            doc_list.append(filter_list)\n",
    "        f.close()\n",
    "    return doc_list  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, keyword_num):\n",
    "        \"\"\"\n",
    "        :param keyword_num: 关键词数量\n",
    "        \"\"\"\n",
    "        self.keyword_num = keyword_num\n",
    "    def fit(self, doc_list):\n",
    "        \"\"\"\n",
    "        根据训练语料，得到词的idf值\n",
    "        :param doc_list: 训练语料\n",
    "        :return: 包含语料各个词的idf字典，以及未登录词的默认idf值\n",
    "        \"\"\"\n",
    "        idf_dic = defaultdict(int)\n",
    "        tt_count = 0\n",
    "        for word_list in doc_list:\n",
    "            for word in set(word_list):\n",
    "                idf_dic[word] += 1\n",
    "            tt_count += 1\n",
    "\n",
    "        self.idf_dic = {word: math.log(tt_count / (num + 1.0)) for word, num in idf_dic.items()}\n",
    "        self.default_idf = math.log(tt_count / 1.0)\n",
    "    \n",
    "    def get_keyword(self, word_list):\n",
    "        \"\"\"\n",
    "        对目标词列表进行tf-idf转换\n",
    "        :param word_list: 目标词列表\n",
    "        \"\"\"\n",
    "        # 统计词频\n",
    "        tf_dic = defaultdict(int)\n",
    "        for word in word_list:\n",
    "            tf_dic[word] += 1\n",
    "        \n",
    "        self.tfidf_dic = {word: tf * self.idf_dic.get(word, self.default_idf) for word, tf in tf_dic.items()}\n",
    "        sorted_words = sorted(list(self.tfidf_dic.items()), key=lambda tup: tup[1], reverse=True)\n",
    "        keywords = sorted_words[:self.keyword_num]\n",
    "        for i, (word, score) in enumerate(keywords):\n",
    "            print(\"%d: %s %.4f\" % (i, word, score))\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel:\n",
    "    def __init__(self, keyword_num, num_topics=4):\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "    def fit(self, doc_list):\n",
    "        doc_list = [\" \".join(doc) for doc in doc_list]\n",
    "        self.tfidf = TfidfVectorizer(min_df=0)\n",
    "        vec = self.tfidf.fit_transform(doc_list)\n",
    "        self.model = LatentDirichletAllocation(n_components=self.num_topics, random_state=0)\n",
    "        self.model.fit(vec)\n",
    "    def __normalize(self, arr):\n",
    "        \"\"\"\n",
    "        对目标向量 / 矩阵进行二范数规范化\n",
    "        \"\"\"\n",
    "        norm = np.linalg.norm(arr, 2, axis=1, keepdims=True)\n",
    "        return arr / norm\n",
    "    def get_keyword(self, word_list):\n",
    "        \"\"\"\n",
    "        以每个词单独作为文档，计算其与整体语料主题分布的相似度（cosine），并按相似度高低选出关键词\n",
    "        \"\"\"\n",
    "        content = \" \".join(word_list)\n",
    "        words = list(set(word_list))  # m\n",
    "        seq_vec = self.tfidf.transform([content])  # sparsed  1 * n\n",
    "        seq_topic_vec = self.model.transform(seq_vec)  # 1 * num_topis 语句的主题分布\n",
    "        word_mat = self.tfidf.transform(words)  # sparsed m * n\n",
    "        word_topic_mat = self.model.transform(word_mat)\n",
    "        \n",
    "        normed_seq_topic_vec = self.__normalize(seq_topic_vec)  # 1 * n\n",
    "        normed_word_topic_mat = self.__normalize(word_topic_mat)  # m * n\n",
    "        \n",
    "        sims = np.dot(normed_word_topic_mat, normed_seq_topic_vec.T).flatten()\n",
    "        \n",
    "        sorted_words = sorted(list(zip(words, sims)), key=lambda tup: tup[1], reverse=True)\n",
    "        keywords =  sorted_words[:self.keyword_num]\n",
    "        for i, (word, score) in enumerate(keywords):\n",
    "            print(\"%d: %s %.4f\" % (i, word, score))\n",
    "        return keywords\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extract(text, train_corpus=None, keyword_num=10, method=\"tfidf\", seg_pos=False):\n",
    "    \"\"\"\n",
    "    统一关键词抽取API\n",
    "    :param text: 目标语料集\n",
    "    :param train_corpus: 训练集，用于抽取idf值\n",
    "    :param keyword_num: 最终抽取的关键词数量\n",
    "    :param method: 具体抽取的算法，有tf-idf, text-rank, lda算法\n",
    "    :param seg_pos: 筛选关键词词性的方法，如果为True，则只保留名词\n",
    "    \"\"\"\n",
    "    stop_word_list = get_stopword_list()\n",
    "    seg_list = seg_to_list(text, seg_pos)\n",
    "    filter_list = word_filter(seg_list, stop_word_list)\n",
    "    \n",
    "    if method.lower() in [\"tfidf\", \"tf-idf\", \"tf_idf\"]:\n",
    "        if train_corpus is None:\n",
    "            warnings.warn(\"train corpus is None. Loaded from default path!\")\n",
    "            train_corpus = load_data(path + \"corpus.txt\", seg_pos)\n",
    "        model = TFIDF(keyword_num)\n",
    "        model.fit(train_corpus)\n",
    "        keywords = model.get_keyword(filter_list)\n",
    "    elif method.lower() in [\"textrank\", \"text rank\", \"text-rank\", \"text_rank\"]:\n",
    "        params = {\"topK\": keyword_num, \"sentence\": text, \"withWeight\": True}\n",
    "        params[\"allowPOS\"] = [\"n\", \"ns\", \"nz\"] if seg_pos else [\"n\", \"ns\", \"nz\", \"vn\", \"v\"]\n",
    "        keywords = analyse.textrank(**params)\n",
    "        for i, (word, score) in enumerate(keywords):\n",
    "            print(\"%d: %s %.4f\" % (i, word, score))\n",
    "    elif method.lower() in [\"lda\", \"topic\"]:\n",
    "        if train_corpus is None:\n",
    "            warnings.warn(\"train corpus is None. Loaded from default path!\")\n",
    "            train_corpus = load_data(path + \"corpus.txt\", seg_pos)\n",
    "        model = TopicModel(keyword_num)\n",
    "        model.fit(train_corpus)\n",
    "        keywords = model.get_keyword(filter_list)\n",
    "    else:\n",
    "        raise Exception(\"method %s is invalid!\" % method)\n",
    "    return keywords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "       '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "       '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "       '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "       '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "       '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "       '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "       '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "       '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "       '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "       '常委会主任陈健倩介绍了大会的筹备情况。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5p/db2l5pk14pd2d9bwhhtqbrgc0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.765 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: train corpus is None. Loaded from default path!\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 晋江市 22.0336\n",
      "1: 救助 12.6248\n",
      "2: 城市 12.0529\n",
      "3: 大会 12.0038\n",
      "4: 爱心 11.6364\n",
      "5: 中华 10.1397\n",
      "6: 基金会 9.8219\n",
      "7: 许嘉璐 8.8134\n",
      "8: 巡视员 8.8134\n",
      "9: 重庆市 8.8134\n",
      "\n",
      "text-rank\n",
      "0: 城市 1.0000\n",
      "1: 救助 0.8287\n",
      "2: 爱心 0.8102\n",
      "3: 中国 0.7955\n",
      "4: 社会 0.7491\n",
      "5: 中华 0.7053\n",
      "6: 基金会 0.6890\n",
      "7: 晋江市 0.6873\n",
      "8: 大会 0.5803\n",
      "9: 介绍 0.5147\n",
      "\n",
      "topic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: train corpus is None. Loaded from default path!\n",
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 捐赠 0.9605\n",
      "1: 基金会 0.9605\n",
      "2: 项目 0.9604\n",
      "3: 年度 0.9604\n",
      "4: 活动 0.9603\n",
      "5: 行动 0.9603\n",
      "6: 中华 0.9601\n",
      "7: 爱心 0.9599\n",
      "8: 老龄 0.9597\n",
      "9: 2012 0.9596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in [\"tfidf\", \"text-rank\", \"topic\"]:\n",
    "    print(method)\n",
    "    keyword_extract(text, keyword_num=10, method=method, seg_pos=False)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: train corpus is None. Loaded from default path!\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 晋江市 22.0336\n",
      "1: 城市 12.0529\n",
      "2: 大会 12.0038\n",
      "3: 爱心 11.6364\n",
      "4: 中华 10.1397\n",
      "5: 基金会 9.8219\n",
      "6: 许嘉璐 8.8134\n",
      "7: 巡视员 8.8134\n",
      "8: 重庆市 8.8134\n",
      "9: 人大常委会 8.8134\n",
      "\n",
      "text-rank\n",
      "0: 城市 1.0000\n",
      "1: 爱心 0.8025\n",
      "2: 中国 0.7980\n",
      "3: 社会 0.6994\n",
      "4: 基金会 0.6903\n",
      "5: 中华 0.6882\n",
      "6: 晋江市 0.5972\n",
      "7: 公益活动 0.5059\n",
      "8: 大会 0.4981\n",
      "9: 发布会 0.4191\n",
      "\n",
      "topic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: train corpus is None. Loaded from default path!\n",
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 民政部 0.9640\n",
      "1: 中华 0.9638\n",
      "2: 基金会 0.9637\n",
      "3: 老龄 0.9636\n",
      "4: 社会 0.9636\n",
      "5: 理事长 0.9627\n",
      "6: 重点 0.9622\n",
      "7: 爱心 0.9622\n",
      "8: 公益活动 0.9620\n",
      "9: 晋江市 0.9619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method in [\"tfidf\", \"text-rank\", \"topic\"]:\n",
    "    print(method)\n",
    "    keyword_extract(text, keyword_num=10, method=method, seg_pos=True)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
