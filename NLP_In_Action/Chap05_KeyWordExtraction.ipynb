{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from _utils import u_constant\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "path = u_constant.PATH_ROOT + \"for learn/Python/NLP_in_Action/chapter-5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():\n",
    "    \"\"\"\n",
    "    加载停用词\n",
    "    \"\"\"\n",
    "    file_path = path + \"stopword.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        l = [s.replace(\"\\n\", \"\") for s in f.readlines()]\n",
    "        f.close()\n",
    "    return l\n",
    "\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    \"\"\"\n",
    "    分词方法，调用jieba接口\n",
    "    input:\n",
    "    - sentence: 句子, string\n",
    "    - pos: 是否采用词性标注分词, boolean\n",
    "    \"\"\"\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.lcut(sentence)\n",
    "    else:\n",
    "        # 采用词性标注的分词方法\n",
    "        seg_list = psg.lcut(sentence)\n",
    "    return seg_list\n",
    "        \n",
    "def word_filter(seg_list, stopword_list):\n",
    "    \"\"\"\n",
    "    去除干扰词\n",
    "    input:\n",
    "    - seg_list: 分词列表，generator[String]或generator[(String, String)]\n",
    "    - stopword_list: 停用词列表， List[String]\n",
    "    \"\"\"\n",
    "    filter_list = []\n",
    "    # 由于分词列表可能是由词性标注生成的，因此接下来处理时需要判断\n",
    "    pos = isinstance(seg_list[0], tuple)\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "        else:\n",
    "            word, flag = seg\n",
    "            # 只保留名词\n",
    "            if not flag.startswith(\"n\"):\n",
    "                continue\n",
    "        # 过滤掉长度小于2的词 和 停用词\n",
    "        if (len(word) < 2) | (word in stopword_list):\n",
    "            continue\n",
    "        \n",
    "        filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "def load_data(corpus_path, pos=False):\n",
    "    \"\"\"\n",
    "    加载语料数据\n",
    "    input: \n",
    "    - corpus_path: 语料地址\n",
    "    - pos: 决定是否用词性分词\n",
    "    :return: 语料二重列表， 第一重为语料，第二重为该语料的词（已去除停用词），如果pos为True，则为（词、词性）tuple\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    stopword_list = get_stopword_list()\n",
    "    with open(corpus_path, \"r\", encoding=\"UTf-8\") as f:\n",
    "        for line in f:\n",
    "            content = line.strip()\n",
    "            seg_list = seg_to_list(content, pos)\n",
    "            filter_list = word_filter(seg_list, stopword_list)\n",
    "            doc_list.append(filter_list)\n",
    "        f.close()\n",
    "    return doc_list  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, keyword_num):\n",
    "        \"\"\"\n",
    "        :param keyword_num: 关键词数量\n",
    "        \"\"\"\n",
    "        self.keyword_num = keyword_num\n",
    "    def fit(self, doc_list):\n",
    "        \"\"\"\n",
    "        根据训练语料，得到词的idf值\n",
    "        :param doc_list: 训练语料\n",
    "        :return: 包含语料各个词的idf字典，以及未登录词的默认idf值\n",
    "        \"\"\"\n",
    "        idf_dic = defaultdict(int)\n",
    "        tt_count = 0\n",
    "        for word_list in doc_list:\n",
    "            for word in set(word_list):\n",
    "                idf_dic[word] += 1\n",
    "            tt_count += 1\n",
    "\n",
    "        self.idf_dic = {word: math.log(tt_count / (num + 1.0)) for word, num in idf_dic.items()}\n",
    "        self.default_idf = math.log(tt_count / 1.0)\n",
    "    \n",
    "    def get_keyword(self, word_list):\n",
    "        \"\"\"\n",
    "        对目标词列表进行tf-idf转换\n",
    "        :param word_list: 目标词列表\n",
    "        \"\"\"\n",
    "        # 统计词频\n",
    "        tf_dic = defaultdict(int)\n",
    "        for word in word_list:\n",
    "            tf_dic[word] += 1\n",
    "        \n",
    "        self.tfidf_dic = {word: tf * self.idf_dic.get(word, self.default_idf) for word, tf in tf_dic.items()}\n",
    "        sorted_words = sorted(list(self.tfidf_dic.items()), key=lambda tup: tup[1], reverse=True)\n",
    "        keywords = sorted_words[:self.keyword_num]\n",
    "        for i, (word, score) in enumerate(keywords):\n",
    "            print(\"%d: %s %.4f\" % (i, word, score))\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel:\n",
    "    def __init__(self, keyword_num, num_topics=4):\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "    def fit(self, doc_list):\n",
    "        doc_list = [\" \".join(doc) for doc in doc_list]\n",
    "        self.tfidf = TfidfVectorizer(min_df=0)\n",
    "        vec = self.tfidf.fit_transform(doc_list)\n",
    "        self.model = LatentDirichletAllocation(n_components=self.num_topics, random_state=0)\n",
    "        self.model.fit(vec)\n",
    "    def __normalize(self, arr):\n",
    "        \"\"\"\n",
    "        对目标向量 / 矩阵进行二范数规范化\n",
    "        \"\"\"\n",
    "        norm = np.linalg.norm(arr, 2, axis=1, keepdims=True)\n",
    "        return arr / norm\n",
    "    def get_keyword(self, word_list):\n",
    "        \"\"\"\n",
    "        以每个词单独作为文档，计算其与整体语料主题分布的相似度（cosine），并按相似度高低选出关键词\n",
    "        \"\"\"\n",
    "        content = \" \".join(word_list)\n",
    "        words = list(set(word_list))  # m\n",
    "        seq_vec = self.tfidf.transform([content])  # sparsed  1 * n\n",
    "        seq_topic_vec = self.model.transform(seq_vec)  # 1 * num_topis 语句的主题分布\n",
    "        word_mat = self.tfidf.transform(words)  # sparsed m * n\n",
    "        word_topic_mat = self.model.transform(word_mat)\n",
    "        \n",
    "        normed_seq_topic_vec = self.__normalize(seq_topic_vec)  # 1 * n\n",
    "        normed_word_topic_mat = self.__normalize(word_topic_mat)  # m * n\n",
    "        \n",
    "        sims = np.dot(normed_word_topic_mat, normed_seq_topic_vec.T).flatten()\n",
    "        \n",
    "        sorted_words = sorted(list(zip(words, sims)), key=lambda tup: tup[1], reverse=True)\n",
    "        keywords =  sorted_words[:self.keyword_num]\n",
    "        for i, (word, score) in enumerate(keywords):\n",
    "            print(\"%d: %s %.4f\" % (i, word, score))\n",
    "        return keywords\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "       '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "       '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "       '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "       '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "       '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "       '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "       '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "       '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "       '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "       '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "pos = False\n",
    "seg_list = seg_to_list(text, pos)\n",
    "stop_word_list = get_stopword_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = word_filter(seg_list, stop_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = load_data(path + \"corpus.txt\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 晋江市 22.0336\n",
      "1: 救助 12.6248\n",
      "2: 城市 12.0529\n",
      "3: 大会 12.0038\n",
      "4: 爱心 11.6364\n",
      "5: 中华 10.1397\n",
      "6: 基金会 9.8219\n",
      "7: 许嘉璐 8.8134\n",
      "8: 巡视员 8.8134\n",
      "9: 重庆市 8.8134\n"
     ]
    }
   ],
   "source": [
    "tfidf = TFIDF(keyword_num=10)\n",
    "tfidf.fit(doc_list)\n",
    "keywords = tfidf.get_keyword(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ = TfidfVectorizer(min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5e93151f66d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfidf_.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/Users/lixing/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 捐赠 0.9605\n",
      "1: 基金会 0.9605\n",
      "2: 项目 0.9604\n",
      "3: 年度 0.9604\n",
      "4: 活动 0.9603\n",
      "5: 行动 0.9603\n",
      "6: 中华 0.9601\n",
      "7: 爱心 0.9599\n",
      "8: 老龄 0.9597\n",
      "9: 2012 0.9596\n"
     ]
    }
   ],
   "source": [
    "topic = TopicModel(keyword_num=10)\n",
    "topic.fit(doc_list)\n",
    "keywords = topic.get_keyword(filter_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
