{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from _utils import u_constant\n",
    "path = u_constant.PATH_ROOT + \"for learn/Python/NLP_in_Action/chapter-8/sentiment-analysis/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 读取词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_data():\n",
    "    word_list = np.load(path + \"wordsList.npy\")\n",
    "    word_list = [word.decode(\"utf-8\") for word in word_list]\n",
    "    return word_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "word_list = load_word_data()\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有重复元素 '0'\n",
    "word_map = {word: index for index, word in enumerate(word_list)}  # key: word, value: word_index\n",
    "# len(word_map)  # 399999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 原始数据处理，生成TF训练样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    \"\"\"\n",
    "    读取文件夹下的训练样本文件，并整合为一个完整的数据集\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for _type in [\"neg\", \"pos\"]:\n",
    "        label = 1 if _type == \"pos\" else 0\n",
    "        folder_path = path + _type + \"/\"\n",
    "        files = [folder_path + file for file in os.listdir(folder_path) if os.path.isfile(folder_path + file)]\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = \" \".join(f.readlines()).strip()\n",
    "                words_num = len(text.split())\n",
    "                result.append((text, words_num, label))\n",
    "                f.close()\n",
    "        print(\"Load %s finished!\" % _type)\n",
    "    \n",
    "    return pd.DataFrame(result, columns=[\"text\", \"words_num\", \"label\"])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load neg finished!\n",
      "Load pos finished!\n",
      "                                                text  words_num  label\n",
      "0  Story of a man who has unnatural feelings for ...        112      0\n",
      "1  Airport '77 starts as a brand new luxury 747 p...        801      0\n",
      "2  This film lacked something I couldn't put my f...        141      0\n",
      "3  Sorry everyone,,, I know this is supposed to b...        154      0\n",
      "4  When I was little my parents took me along to ...        395      0\n",
      "1    12500\n",
      "0    12500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = load_corpus()\n",
    "print(data[:5])\n",
    "print(data[\"label\"].value_counts())\n",
    "data[[\"text\", \"label\"]].to_csv(path + \"data.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 探查词数分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data[\"words_num\"], bins=50)\n",
    "plt.xlabel(\"word num\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.xlim(0, 1200)\n",
    "plt.ylim(0, 8000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 生成索引矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "MAX_SEQ_LEN = 300\n",
    "UNK_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAT_Generator:\n",
    "    \n",
    "    def __init__(self, word_map, clean_pattern, max_seq_len, unk_id):\n",
    "        self.word_map = word_map\n",
    "        self.pattern = re.compile(clean_pattern)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.unk_id = unk_id\n",
    "    \n",
    "    def clean_sentence(self, string):\n",
    "        \"\"\"\n",
    "        清洗句子\n",
    "        \"\"\"\n",
    "        string = string.lower().replace(\"<br />\", \" \")\n",
    "        return self.pattern.sub(\"\", string)\n",
    "    \n",
    "    def generate(self, texts):\n",
    "        \"\"\"\n",
    "        为语料生成词索引矩阵\n",
    "        :param texts: 语料集\n",
    "        :return: 索引矩阵 (#texts, max_seq_len)\n",
    "        \"\"\"\n",
    "        num_files = len(texts)\n",
    "        # 矩阵初始化，全部元素都为缺省索引\n",
    "        ids = np.full((num_files, self.max_seq_len), self.unk_id, dtype=np.int32)\n",
    "        \n",
    "        # 逐样本遍历\n",
    "        for i, text in enumerate(texts):\n",
    "            clean_text = self.clean_sentence(text)\n",
    "            # 逐词遍历\n",
    "            for j, word in enumerate(clean_text.split()):\n",
    "                if j >= self.max_seq_len:\n",
    "                    break\n",
    "                try:\n",
    "                    ids[i][j] = word_map[word]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "[[    91 201534   2807 ...      0      0      0]\n",
      " [     7    353    297 ...      0      0      0]\n",
      " [    58      3     64 ...      0      0      0]\n",
      " ...\n",
      " [    36   4855    102 ...      0      0      0]\n",
      " [  3202    192   1533 ...    285    518      7]\n",
      " [    37   1005     14 ...      0      0      0]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 打乱顺序\n",
    "def one_hot(x, cate_num=None):\n",
    "    \"\"\"\n",
    "    对一维数组进行one-hot\n",
    "    :param cate_num: 类别数\n",
    "    \"\"\"\n",
    "    x = np.asarray(x).astype(int)\n",
    "    size = cate_num if cate_num is not None else max(x)\n",
    "    return (np.arange(size) == x[:, np.newaxis]).astype(int)\n",
    "texts = data[\"text\"].values\n",
    "labels = one_hot(data[\"label\"].values, 2)\n",
    "texts, labels = shuffle(texts, labels, random_state=0)\n",
    "\n",
    "generator = MAT_Generator(word_map=word_map, clean_pattern=PATTERN, max_seq_len=MAX_SEQ_LEN, unk_id=UNK_ID)\n",
    "ids_mat = generator.generate(texts)\n",
    "\n",
    "print(ids_mat.shape)\n",
    "print(ids_mat[:10])\n",
    "print(labels[:10])\n",
    "np.save(path + \"ids_mat\", ids_mat)\n",
    "np.save(path + \"labels\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 构建BATCH生成方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500\n"
     ]
    }
   ],
   "source": [
    "ids_mat = np.load(path + \"ids_mat.npy\")\n",
    "labels = np.load(path + \"labels.npy\")\n",
    "word_vectors = np.load(path + \"wordVectors.npy\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(ids_mat, labels, test_size=0.1, random_state=0)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lstm_units = 64\n",
    "NUM_LABELS = 2\n",
    "MAX_SEQ_LEN = 300\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():    \n",
    "    X = tf.placeholder(dtype=tf.int32, shape=[None, MAX_SEQ_LEN], name=\"inputs\")\n",
    "    y_ = tf.placeholder(dtype=tf.int32, shape=[None, NUM_LABELS], name=\"labels\")\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(word_vectors, X, name=\"embedded_vectors\")\n",
    "    \n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_units, activation=tf.nn.tanh)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    # RNN output: shape: [batch_size, max_seq_len, cell_num]\n",
    "    value, _ = tf.nn.dynamic_rnn(cell=lstmCell, inputs=embed, dtype=tf.float32)\n",
    "    # [lstm_units, num_labels]\n",
    "    weight = tf.Variable(initial_value=tf.truncated_normal([lstm_units, NUM_LABELS]))\n",
    "    # [-1, num_labels]\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[1, NUM_LABELS]))\n",
    "    # [max_seq_len, batch_size, lstm_units]\n",
    "    value = tf.transpose(value, [1, 0, 2])  \n",
    "    \n",
    "    # 取最后一个输出 [batch_size, lstm_units]\n",
    "    last = tf.gather(params=value, indices=int(value.get_shape()[0] - 1), axis=0)  \n",
    "    \n",
    "    y = last @ weight + bias  # logit 不用 softmax进行计算，都包裹在cross_entropy_with_logit中了\n",
    "    correct_pred = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32), name=\"ACC\")\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    tf.summary.scalar(\"ACC\", accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    生成batch\n",
    "    \"\"\"\n",
    "    assert len(X) == len(y), \"features num doesn't match labels num!\"\n",
    "    total_length = (len(y) // batch_size) * batch_size\n",
    "    X = X[:total_length]\n",
    "    y = y[:total_length]\n",
    "    \n",
    "    for curr in range(0, total_length, batch_size):\n",
    "        batchx = X[curr : curr + batch_size]\n",
    "        batchy = y[curr : curr + batch_size]\n",
    "        yield batchx, batchy    \n",
    "        \n",
    "def get_iternum_from_ckpt(ckpt_path):\n",
    "    with open(ckpt_path + \"checkpoint\", \"r\", encoding=\"utf-8\") as f:\n",
    "        model_info = f.__next__().strip().rstrip('\"')\n",
    "        iters = model_info.split(\".ckpt-\")[1]\n",
    "        return int(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from F:/for learn/Python/NLP_in_Action/chapter-8/sentiment-analysis/models/lstm.ckpt-2700\n",
      "Iterations: 3000  Avg. Train_loss: 2.0320  0.7749 sec / batch test acc iter 3000: 0.5988\n",
      "Iterations: 3500  Avg. Train_loss: 3.2079  1.2530 sec / batch test acc iter 3500: 0.6836\n",
      "Iterations: 4000  Avg. Train_loss: 3.3792  1.3285 sec / batch test acc iter 4000: 0.5524\n",
      "Iterations: 4500  Avg. Train_loss: 3.3660  1.3335 sec / batch test acc iter 4500: 0.5088\n",
      "Iterations: 5000  Avg. Train_loss: 3.4085  1.1391 sec / batch test acc iter 5000: 0.5472\n",
      "Iterations: 5500  Avg. Train_loss: 2.9512  1.4037 sec / batch test acc iter 5500: 0.7416\n",
      "Iterations: 6000  Avg. Train_loss: 3.0305  1.2063 sec / batch test acc iter 6000: 0.6712\n",
      "Iterations: 6500  Avg. Train_loss: 3.1944  1.2001 sec / batch test acc iter 6500: 0.6968\n",
      "Iterations: 7000  Avg. Train_loss: 2.9005  1.2243 sec / batch test acc iter 7000: 0.7048\n",
      "Iterations: 7500  Avg. Train_loss: 3.1034  1.2060 sec / batch test acc iter 7500: 0.6760\n",
      "Iterations: 8000  Avg. Train_loss: 3.0306  1.1707 sec / batch test acc iter 8000: 0.7636\n",
      "Iterations: 8500  Avg. Train_loss: 2.9714  1.1771 sec / batch test acc iter 8500: 0.6164\n",
      "Iterations: 9000  Avg. Train_loss: 3.3766  1.1589 sec / batch test acc iter 9000: 0.7276\n",
      "Iterations: 9500  Avg. Train_loss: 2.9032  1.1374 sec / batch test acc iter 9500: 0.7228\n",
      "Iterations: 10000  Avg. Train_loss: 2.8563  1.1407 sec / batch test acc iter 10000: 0.7228\n",
      "Iterations: 10500  Avg. Train_loss: 2.9262  1.1970 sec / batch test acc iter 10500: 0.6872\n",
      "Iterations: 11000  Avg. Train_loss: 2.8745  1.2000 sec / batch test acc iter 11000: 0.7100\n",
      "Iterations: 11500  Avg. Train_loss: 2.8622  1.1291 sec / batch test acc iter 11500: 0.6304\n",
      "Iterations: 12000  Avg. Train_loss: 2.6780  1.1263 sec / batch test acc iter 12000: 0.7384\n",
      "Iterations: 12500  Avg. Train_loss: 2.8459  1.2859 sec / batch test acc iter 12500: 0.7292\n",
      "Iterations: 13000  Avg. Train_loss: 2.7675  1.1420 sec / batch test acc iter 13000: 0.7584\n",
      "Iterations: 13500  Avg. Train_loss: 3.3126  1.0915 sec / batch test acc iter 13500: 0.5684\n",
      "Iterations: 14000  Avg. Train_loss: 3.1336  1.1009 sec / batch test acc iter 14000: 0.7268\n",
      "Iterations: 14500  Avg. Train_loss: 2.7786  1.2232 sec / batch test acc iter 14500: 0.6872\n",
      "Iterations: 15000  Avg. Train_loss: 2.7695  1.3926 sec / batch test acc iter 15000: 0.7148\n",
      "Iterations: 15500  Avg. Train_loss: 2.7558  1.2267 sec / batch test acc iter 15500: 0.7764\n",
      "Iterations: 16000  Avg. Train_loss: 2.9100  1.2301 sec / batch test acc iter 16000: 0.7260\n",
      "Iterations: 16500  Avg. Train_loss: 2.7049  1.2492 sec / batch test acc iter 16500: 0.6712\n",
      "Iterations: 17000  Avg. Train_loss: 2.7692  1.2921 sec / batch test acc iter 17000: 0.7756\n",
      "Iterations: 17500  Avg. Train_loss: 2.4857  1.2246 sec / batch test acc iter 17500: 0.7780\n",
      "Iterations: 18000  Avg. Train_loss: 2.4300  1.6411 sec / batch test acc iter 18000: 0.7892\n",
      "Iterations: 18500  Avg. Train_loss: 2.2949  1.6279 sec / batch test acc iter 18500: 0.8020\n",
      "Iterations: 19000  Avg. Train_loss: 2.4010  1.4661 sec / batch test acc iter 19000: 0.7016\n",
      "Iterations: 19500  Avg. Train_loss: 2.3698  1.2596 sec / batch test acc iter 19500: 0.7972\n",
      "Iterations: 20000  Avg. Train_loss: 2.0876  1.2116 sec / batch test acc iter 20000: 0.8088\n",
      "Iterations: 20500  Avg. Train_loss: 1.9876  1.1305 sec / batch test acc iter 20500: 0.8232\n",
      "Iterations: 21000  Avg. Train_loss: 1.9267  1.1572 sec / batch test acc iter 21000: 0.8212\n",
      "Iterations: 21500  Avg. Train_loss: 1.8754  1.1386 sec / batch test acc iter 21500: 0.8324\n",
      "Iterations: 22000  Avg. Train_loss: 1.7887  1.1355 sec / batch test acc iter 22000: 0.8288\n",
      "Iterations: 22500  Avg. Train_loss: 1.7585  1.1563 sec / batch test acc iter 22500: 0.8360\n",
      "Iterations: 23000  Avg. Train_loss: 1.6875  1.1189 sec / batch test acc iter 23000: 0.8348\n",
      "Iterations: 23500  Avg. Train_loss: 1.6311  1.1437 sec / batch test acc iter 23500: 0.8436\n",
      "Iterations: 24000  Avg. Train_loss: 1.5766  1.1454 sec / batch test acc iter 24000: 0.8420\n",
      "Iterations: 24500  Avg. Train_loss: 1.5191  1.2173 sec / batch test acc iter 24500: 0.8368\n",
      "Iterations: 25000  Avg. Train_loss: 1.4708  1.1915 sec / batch test acc iter 25000: 0.8364\n",
      "Iterations: 25500  Avg. Train_loss: 1.4160  1.1349 sec / batch test acc iter 25500: 0.8468\n",
      "Iterations: 26000  Avg. Train_loss: 1.3680  1.1403 sec / batch test acc iter 26000: 0.8384\n",
      "Iterations: 26500  Avg. Train_loss: 1.2896  1.1186 sec / batch test acc iter 26500: 0.8444\n",
      "Iterations: 27000  Avg. Train_loss: 1.2577  1.1813 sec / batch test acc iter 27000: 0.8528\n",
      "Iterations: 27500  Avg. Train_loss: 1.1905  1.1346 sec / batch test acc iter 27500: 0.8416\n",
      "Iterations: 28000  Avg. Train_loss: 1.1356  1.2325 sec / batch test acc iter 28000: 0.8480\n",
      "Iterations: 28500  Avg. Train_loss: 1.0830  1.1259 sec / batch test acc iter 28500: 0.8364\n",
      "Iterations: 29000  Avg. Train_loss: 1.0621  1.1688 sec / batch test acc iter 29000: 0.8528\n",
      "Iterations: 29500  Avg. Train_loss: 1.0266  1.1990 sec / batch test acc iter 29500: 0.8420\n",
      "Iterations: 30000  Avg. Train_loss: 0.9389  1.3834 sec / batch test acc iter 30000: 0.8296\n",
      "Iterations: 30500  Avg. Train_loss: 0.9502  1.1798 sec / batch test acc iter 30500: 0.8328\n",
      "Iterations: 31000  Avg. Train_loss: 0.8731  1.2185 sec / batch test acc iter 31000: 0.8416\n",
      "Iterations: 31500  Avg. Train_loss: 0.8461  1.4798 sec / batch test acc iter 31500: 0.8460\n",
      "Iterations: 32000  Avg. Train_loss: 0.8249  1.2937 sec / batch test acc iter 32000: 0.8316\n",
      "Iterations: 32500  Avg. Train_loss: 0.8621  1.4319 sec / batch test acc iter 32500: 0.8388\n",
      "Iterations: 33000  Avg. Train_loss: 0.7803  1.4030 sec / batch test acc iter 33000: 0.8260\n",
      "Iterations: 33500  Avg. Train_loss: 0.7229  1.3116 sec / batch test acc iter 33500: 0.8364\n",
      "Iterations: 34000  Avg. Train_loss: 0.6960  1.3562 sec / batch test acc iter 34000: 0.8260\n",
      "Iterations: 34500  Avg. Train_loss: 0.6835  1.2546 sec / batch test acc iter 34500: 0.8328\n",
      "Iterations: 35000  Avg. Train_loss: 0.6942  1.2148 sec / batch test acc iter 35000: 0.8256\n",
      "Iterations: 35500  Avg. Train_loss: 0.6285  1.2503 sec / batch test acc iter 35500: 0.8300\n",
      "Iterations: 36000  Avg. Train_loss: 0.6390  1.2561 sec / batch test acc iter 36000: 0.8384\n",
      "Iterations: 36500  Avg. Train_loss: 0.6148  1.3213 sec / batch test acc iter 36500: 0.8272\n",
      "Iterations: 37000  Avg. Train_loss: 0.5616  1.2877 sec / batch test acc iter 37000: 0.8340\n",
      "Iterations: 37500  Avg. Train_loss: 0.5072  1.3206 sec / batch test acc iter 37500: 0.8372\n",
      "Iterations: 38000  Avg. Train_loss: 0.5189  1.3147 sec / batch test acc iter 38000: 0.8336\n",
      "Iterations: 38500  Avg. Train_loss: 0.5321  1.3407 sec / batch test acc iter 38500: 0.8292\n",
      "Iterations: 39000  Avg. Train_loss: 0.4772  1.2966 sec / batch test acc iter 39000: 0.8408\n",
      "Iterations: 39500  Avg. Train_loss: 0.4676  1.3511 sec / batch test acc iter 39500: 0.8280\n",
      "Iterations: 40000  Avg. Train_loss: 0.4553  1.3875 sec / batch test acc iter 40000: 0.8284\n",
      "Iterations: 40500  Avg. Train_loss: 0.4383  1.4055 sec / batch test acc iter 40500: 0.8288\n",
      "Iterations: 41000  Avg. Train_loss: 0.4000  1.3181 sec / batch test acc iter 41000: 0.8092\n",
      "Iterations: 41500  Avg. Train_loss: 0.3945  1.3038 sec / batch test acc iter 41500: 0.8360\n",
      "Iterations: 42000  Avg. Train_loss: 0.4221  1.2289 sec / batch test acc iter 42000: 0.8228\n",
      "Iterations: 42500  Avg. Train_loss: 0.3542  1.2643 sec / batch test acc iter 42500: 0.8400\n",
      "Iterations: 43000  Avg. Train_loss: 0.3898  1.3190 sec / batch test acc iter 43000: 0.8220\n",
      "Iterations: 43500  Avg. Train_loss: 0.3352  1.3402 sec / batch test acc iter 43500: 0.8288\n",
      "Iterations: 44000  Avg. Train_loss: 0.3209  1.1746 sec / batch test acc iter 44000: 0.8280\n",
      "Iterations: 44500  Avg. Train_loss: 0.3581  1.2333 sec / batch test acc iter 44500: 0.8328\n",
      "Iterations: 45000  Avg. Train_loss: 0.3125  1.1328 sec / batch test acc iter 45000: 0.8312\n",
      "Iterations: 45500  Avg. Train_loss: 0.3300  1.1127 sec / batch test acc iter 45500: 0.8236\n",
      "Iterations: 46000  Avg. Train_loss: 0.2877  1.1256 sec / batch test acc iter 46000: 0.8200\n",
      "Iterations: 46500  Avg. Train_loss: 0.2844  1.1139 sec / batch test acc iter 46500: 0.8308\n",
      "Iterations: 47000  Avg. Train_loss: 0.3299  1.1374 sec / batch test acc iter 47000: 0.8360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 47500  Avg. Train_loss: 0.2815  1.1310 sec / batch test acc iter 47500: 0.8232\n",
      "Iterations: 48000  Avg. Train_loss: 0.3104  1.1102 sec / batch test acc iter 48000: 0.8364\n",
      "Iterations: 48500  Avg. Train_loss: 0.2661  1.1019 sec / batch test acc iter 48500: 0.8344\n",
      "Iterations: 49000  Avg. Train_loss: 0.2674  1.0910 sec / batch test acc iter 49000: 0.8292\n",
      "Iterations: 49500  Avg. Train_loss: 0.2956  1.0928 sec / batch test acc iter 49500: 0.8160\n",
      "Iterations: 50000  Avg. Train_loss: 0.2449  1.1128 sec / batch test acc iter 50000: 0.8260\n",
      "Iterations: 50500  Avg. Train_loss: 0.2576  1.1011 sec / batch test acc iter 50500: 0.8280\n",
      "Iterations: 51000  Avg. Train_loss: 0.2375  1.1038 sec / batch test acc iter 51000: 0.8256\n",
      "Iterations: 51500  Avg. Train_loss: 0.2280  1.1853 sec / batch test acc iter 51500: 0.8260\n",
      "Iterations: 52000  Avg. Train_loss: 0.2479  1.1021 sec / batch test acc iter 52000: 0.8308\n",
      "Iterations: 52500  Avg. Train_loss: 0.2574  1.1125 sec / batch test acc iter 52500: 0.8256\n",
      "Iterations: 53000  Avg. Train_loss: 0.2858  1.1130 sec / batch test acc iter 53000: 0.8224\n",
      "Iterations: 53500  Avg. Train_loss: 0.2422  1.1382 sec / batch test acc iter 53500: 0.8288\n",
      "Iterations: 54000  Avg. Train_loss: 0.2593  1.1014 sec / batch test acc iter 54000: 0.8220\n",
      "Iterations: 54500  Avg. Train_loss: 0.2182  1.1181 sec / batch test acc iter 54500: 0.8276\n",
      "Iterations: 55000  Avg. Train_loss: 0.2133  1.1371 sec / batch test acc iter 55000: 0.8164\n",
      "Iterations: 55500  Avg. Train_loss: 0.2857  1.1530 sec / batch test acc iter 55500: 0.8284\n",
      "Iterations: 56000  Avg. Train_loss: 0.1998  1.4046 sec / batch test acc iter 56000: 0.8328\n",
      "Iterations: 56500  Avg. Train_loss: 0.2384  1.4242 sec / batch test acc iter 56500: 0.8284\n",
      "Iterations: 57000  Avg. Train_loss: 0.1868  1.3931 sec / batch test acc iter 57000: 0.8176\n",
      "Iterations: 57500  Avg. Train_loss: 0.2230  1.4351 sec / batch test acc iter 57500: 0.8304\n",
      "Iterations: 58000  Avg. Train_loss: 0.2133  1.3556 sec / batch test acc iter 58000: 0.8352\n",
      "Iterations: 58500  Avg. Train_loss: 0.2158  1.3411 sec / batch test acc iter 58500: 0.8232\n",
      "Iterations: 59000  Avg. Train_loss: 0.2258  1.2400 sec / batch test acc iter 59000: 0.8316\n",
      "Iterations: 59500  Avg. Train_loss: 0.1601  1.2891 sec / batch test acc iter 59500: 0.8312\n",
      "Iterations: 60000  Avg. Train_loss: 0.1816  1.2643 sec / batch test acc iter 60000: 0.8312\n",
      "Iterations: 60500  Avg. Train_loss: 0.2329  1.2747 sec / batch test acc iter 60500: 0.8364\n",
      "Iterations: 61000  Avg. Train_loss: 0.2052  1.2370 sec / batch test acc iter 61000: 0.8280\n",
      "Iterations: 61500  Avg. Train_loss: 0.1766  1.3505 sec / batch test acc iter 61500: 0.8256\n",
      "Iterations: 62000  Avg. Train_loss: 0.1852  1.4984 sec / batch test acc iter 62000: 0.8280\n",
      "Iterations: 62500  Avg. Train_loss: 0.2048  1.4388 sec / batch test acc iter 62500: 0.8260\n",
      "Iterations: 63000  Avg. Train_loss: 0.1751  1.4493 sec / batch test acc iter 63000: 0.8284\n",
      "Iterations: 63500  Avg. Train_loss: 0.1838  1.4699 sec / batch test acc iter 63500: 0.8364\n",
      "Iterations: 64000  Avg. Train_loss: 0.1938  1.4482 sec / batch test acc iter 64000: 0.8336\n",
      "Iterations: 64500  Avg. Train_loss: 0.1862  1.4568 sec / batch test acc iter 64500: 0.8276\n",
      "Iterations: 65000  Avg. Train_loss: 0.1889  1.4582 sec / batch test acc iter 65000: 0.8180\n",
      "Iterations: 65500  Avg. Train_loss: 0.1641  1.5083 sec / batch test acc iter 65500: 0.8284\n",
      "Iterations: 66000  Avg. Train_loss: 0.1711  1.2791 sec / batch test acc iter 66000: 0.8252\n",
      "Iterations: 66500  Avg. Train_loss: 0.1922  1.2088 sec / batch test acc iter 66500: 0.8340\n",
      "Iterations: 67000  Avg. Train_loss: 0.1960  1.2456 sec / batch test acc iter 67000: 0.8300\n",
      "Iterations: 67500  Avg. Train_loss: 0.1713  1.1916 sec / batch test acc iter 67500: 0.8280\n",
      "Iterations: 68000  Avg. Train_loss: 0.1910  1.2231 sec / batch test acc iter 68000: 0.8256\n",
      "Iterations: 68500  Avg. Train_loss: 0.2005  1.2681 sec / batch test acc iter 68500: 0.8312\n",
      "Iterations: 69000  Avg. Train_loss: 0.1516  1.2469 sec / batch test acc iter 69000: 0.8272\n",
      "Iterations: 69500  Avg. Train_loss: 0.2043  1.2235 sec / batch test acc iter 69500: 0.8348\n",
      "Iterations: 70000  Avg. Train_loss: 0.1770  1.2420 sec / batch test acc iter 70000: 0.8300\n",
      "Iterations: 70500  Avg. Train_loss: 0.1546  1.2326 sec / batch test acc iter 70500: 0.8336\n",
      "Iterations: 71000  Avg. Train_loss: 0.2239  1.2535 sec / batch test acc iter 71000: 0.8260\n",
      "Iterations: 71500  Avg. Train_loss: 0.1861  1.2204 sec / batch test acc iter 71500: 0.8340\n",
      "Iterations: 72000  Avg. Train_loss: 0.1636  1.2119 sec / batch test acc iter 72000: 0.8316\n",
      "Iterations: 72500  Avg. Train_loss: 0.1416  1.2956 sec / batch test acc iter 72500: 0.8240\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 程序最多只能占用指定gpu70%的显存\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "log_path = u_constant.PATH_ROOT + \"board/lstm/\"\n",
    "ckpt_path = path + \"models/\"\n",
    "\n",
    "with tf.Session(graph=train_graph, config=config) as sess:\n",
    "    sess.graph.finalize()\n",
    "    train_writer = tf.summary.FileWriter(log_path + \"train\", graph=sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_path + \"test\") \n",
    "    \n",
    "    if os.path.exists(ckpt_path) and os.path.exists(ckpt_path + \"checkpoint\"):\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n",
    "        iteration = get_iternum_from_ckpt(ckpt_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 1\n",
    "    start = time.time()\n",
    "    avg_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_batches = generate_batch(X_train, y_train, batch_size=batch_size)\n",
    "        for batch_x, batch_y in train_batches:\n",
    "            summary, _, train_loss = sess.run([merged, optimizer, loss], \n",
    "                                                  feed_dict={X: batch_x, y_: batch_y})\n",
    "            avg_loss += train_loss \n",
    "            \n",
    "            if iteration % 10 == 9: # 每 10次记录到 train中\n",
    "                train_writer.add_summary(summary, iteration)              \n",
    "            if iteration % 500 == 0: # 每 500次输出训练进度结果并保存模型\n",
    "                end = time.time()\n",
    "                summary, acc = sess.run([merged, accuracy], \n",
    "                                        feed_dict={X: X_test, y_: y_test})\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                print(\"Iterations: %d \" % (iteration), \n",
    "                      \"Avg. Train_loss: %.4f \" % (avg_loss / 500), \n",
    "                      \"%.4f sec / batch\" % ((end - start) / 500), \n",
    "                      \"test acc iter %d: %.4f\" % (iteration, acc))\n",
    "                \n",
    "                save_path = saver.save(sess, path + \"models/lstm.ckpt\", global_step=iteration)\n",
    "                avg_loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "train_writer.close()\n",
    "test_writer.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
