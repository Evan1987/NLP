{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from _utils import u_constant\n",
    "path = u_constant.PATH_ROOT + \"for learn/Python/NLP_in_Action/chapter-8/sentiment-analysis/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 读取词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_data():\n",
    "    word_list = np.load(path + \"wordsList.npy\")\n",
    "    word_list = [word.decode(\"utf-8\") for word in word_list]\n",
    "    return word_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "word_list = load_word_data()\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有重复元素 '0'\n",
    "word_map = {word: index for index, word in enumerate(word_list)}  # key: word, value: word_index\n",
    "# len(word_map)  # 399999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 原始数据处理，生成TF训练样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    \"\"\"\n",
    "    读取文件夹下的训练样本文件，并整合为一个完整的数据集\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for _type in [\"neg\", \"pos\"]:\n",
    "        label = 1 if _type == \"pos\" else 0\n",
    "        folder_path = path + _type + \"/\"\n",
    "        files = [folder_path + file for file in os.listdir(folder_path) if os.path.isfile(folder_path + file)]\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = \" \".join(f.readlines()).strip()\n",
    "                words_num = len(text.split())\n",
    "                result.append((text, words_num, label))\n",
    "                f.close()\n",
    "        print(\"Load %s finished!\" % _type)\n",
    "    \n",
    "    return pd.DataFrame(result, columns=[\"text\", \"words_num\", \"label\"])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load neg finished!\n",
      "Load pos finished!\n",
      "                                                text  words_num  label\n",
      "0  Story of a man who has unnatural feelings for ...        112      0\n",
      "1  Airport '77 starts as a brand new luxury 747 p...        801      0\n",
      "2  This film lacked something I couldn't put my f...        141      0\n",
      "3  Sorry everyone,,, I know this is supposed to b...        154      0\n",
      "4  When I was little my parents took me along to ...        395      0\n",
      "1    12500\n",
      "0    12500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = load_corpus()\n",
    "print(data[:5])\n",
    "print(data[\"label\"].value_counts())\n",
    "data[[\"text\", \"label\"]].to_csv(path + \"data.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 探查词数分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data[\"words_num\"], bins=50)\n",
    "plt.xlabel(\"word num\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.xlim(0, 1200)\n",
    "plt.ylim(0, 8000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 生成索引矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "MAX_SEQ_LEN = 300\n",
    "UNK_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAT_Generator:\n",
    "    \n",
    "    def __init__(self, word_map, clean_pattern, max_seq_len, unk_id):\n",
    "        self.word_map = word_map\n",
    "        self.pattern = re.compile(clean_pattern)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.unk_id = unk_id\n",
    "    \n",
    "    def clean_sentence(self, string):\n",
    "        \"\"\"\n",
    "        清洗句子\n",
    "        \"\"\"\n",
    "        string = string.lower().replace(\"<br />\", \" \")\n",
    "        return self.pattern.sub(\"\", string)\n",
    "    \n",
    "    def generate(self, texts):\n",
    "        \"\"\"\n",
    "        为语料生成词索引矩阵\n",
    "        :param texts: 语料集\n",
    "        :return: 索引矩阵 (#texts, max_seq_len)\n",
    "        \"\"\"\n",
    "        num_files = len(texts)\n",
    "        # 矩阵初始化，全部元素都为缺省索引\n",
    "        ids = np.full((num_files, self.max_seq_len), self.unk_id, dtype=np.int32)\n",
    "        \n",
    "        # 逐样本遍历\n",
    "        for i, text in enumerate(texts):\n",
    "            clean_text = self.clean_sentence(text)\n",
    "            # 逐词遍历\n",
    "            for j, word in enumerate(clean_text.split()):\n",
    "                if j >= self.max_seq_len:\n",
    "                    break\n",
    "                try:\n",
    "                    ids[i][j] = word_map[word]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "[[    91 201534   2807 ...      0      0      0]\n",
      " [     7    353    297 ...      0      0      0]\n",
      " [    58      3     64 ...      0      0      0]\n",
      " ...\n",
      " [    36   4855    102 ...      0      0      0]\n",
      " [  3202    192   1533 ...    285    518      7]\n",
      " [    37   1005     14 ...      0      0      0]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 打乱顺序\n",
    "def one_hot(x, cate_num=None):\n",
    "    \"\"\"\n",
    "    对一维数组进行one-hot\n",
    "    :param cate_num: 类别数\n",
    "    \"\"\"\n",
    "    x = np.asarray(x).astype(int)\n",
    "    size = cate_num if cate_num is not None else max(x)\n",
    "    return (np.arange(size) == x[:, np.newaxis]).astype(int)\n",
    "texts = data[\"text\"].values\n",
    "labels = one_hot(data[\"label\"].values, 2)\n",
    "texts, labels = shuffle(texts, labels)\n",
    "\n",
    "generator = MAT_Generator(word_map=word_map, clean_pattern=PATTERN, max_seq_len=MAX_SEQ_LEN, unk_id=UNK_ID)\n",
    "ids_mat = generator.generate(texts)\n",
    "\n",
    "print(ids_mat.shape)\n",
    "print(ids_mat[:10])\n",
    "print(labels[:10])\n",
    "np.save(path + \"ids_mat\", ids_mat)\n",
    "np.save(path + \"labels\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 构建BATCH生成方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500\n"
     ]
    }
   ],
   "source": [
    "ids_mat = np.load(path + \"ids_mat.npy\")\n",
    "labels = np.load(path + \"labels.npy\")\n",
    "word_vectors = np.load(path + \"wordVectors.npy\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(ids_mat, labels, test_size=0.1, random_state=0)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lstm_units = 64\n",
    "NUM_LABELS = 2\n",
    "MAX_SEQ_LEN = 300\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():    \n",
    "    X = tf.placeholder(dtype=tf.int32, shape=[None, MAX_SEQ_LEN], name=\"inputs\")\n",
    "    y_ = tf.placeholder(dtype=tf.int32, shape=[None, NUM_LABELS], name=\"labels\")\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(word_vectors, X, name=\"embedded_vectors\")\n",
    "    \n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_units, activation=tf.nn.tanh)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    # RNN output: shape: [batch_size, max_seq_len, cell_num]\n",
    "    value, _ = tf.nn.dynamic_rnn(cell=lstmCell, inputs=embed, dtype=tf.float32)\n",
    "    # [lstm_units, num_labels]\n",
    "    weight = tf.Variable(initial_value=tf.truncated_normal([lstm_units, NUM_LABELS]))\n",
    "    # [-1, num_labels]\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[1, NUM_LABELS]))\n",
    "    # [max_seq_len, batch_size, lstm_units]\n",
    "    value = tf.transpose(value, [1, 0, 2])  \n",
    "    \n",
    "    # 取最后一个输出 [batch_size, lstm_units]\n",
    "    last = tf.gather(params=value, indices=int(value.get_shape()[0] - 1), axis=0)  \n",
    "    \n",
    "    y = last @ weight + bias  # logit 不用 softmax进行计算，都包裹在cross_entropy_with_logit中了\n",
    "    correct_pred = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32), name=\"ACC\")\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    tf.summary.scalar(\"ACC\", accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    生成batch\n",
    "    \"\"\"\n",
    "    assert len(X) == len(y), \"features num doesn't match labels num!\"\n",
    "    total_length = (len(y) // batch_size) * batch_size\n",
    "    X = X[:total_length]\n",
    "    y = y[:total_length]\n",
    "    \n",
    "    for curr in range(0, total_length, batch_size):\n",
    "        batchx = X[curr : curr + batch_size]\n",
    "        batchy = y[curr : curr + batch_size]\n",
    "        yield batchx, batchy    \n",
    "        \n",
    "def get_iternum_from_ckpt(ckpt_path):\n",
    "    with open(ckpt_path + \"checkpoint\", \"r\", encoding=\"utf-8\") as f:\n",
    "        model_info = f.__next__().strip().rstrip('\"')\n",
    "        iters = model_info.split(\".ckpt-\")[1]\n",
    "        return int(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from F:/for learn/Python/NLP_in_Action/chapter-8/sentiment-analysis/models/lstm.ckpt-2200\n",
      "Iterations: 2200  Avg. Train_loss: 0.0066  0.0209 sec / batch test acc iter 2200: 0.5232\n",
      "Iterations: 2300  Avg. Train_loss: 0.6774  0.5414 sec / batch test acc iter 2300: 0.5248\n",
      "Iterations: 2400  Avg. Train_loss: 0.6551  0.6282 sec / batch test acc iter 2400: 0.5972\n",
      "Iterations: 2500  Avg. Train_loss: 0.6606  0.6417 sec / batch test acc iter 2500: 0.6952\n",
      "Iterations: 2600  Avg. Train_loss: 0.6524  0.5684 sec / batch test acc iter 2600: 0.5316\n",
      "Iterations: 2700  Avg. Train_loss: 0.6720  0.6098 sec / batch test acc iter 2700: 0.5664\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 程序最多只能占用指定gpu70%的显存\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "log_path = u_constant.PATH_ROOT + \"board/lstm/\"\n",
    "ckpt_path = path + \"models/\"\n",
    "\n",
    "with tf.Session(graph=train_graph, config=config) as sess:\n",
    "    sess.graph.finalize()\n",
    "    train_writer = tf.summary.FileWriter(log_path + \"train\", graph=sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_path + \"test\") \n",
    "    \n",
    "    if os.path.exists(ckpt_path) and os.path.exists(ckpt_path + \"checkpoint\"):\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n",
    "        iteration = get_iternum_from_ckpt(ckpt_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 1\n",
    "    start = time.time()\n",
    "    avg_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_batches = generate_batch(X_train, y_train, batch_size=batch_size)\n",
    "        for batch_x, batch_y in train_batches:\n",
    "            summary, _, train_loss = sess.run([merged, optimizer, loss], \n",
    "                                                  feed_dict={X: batch_x, y_: batch_y})\n",
    "            avg_loss += train_loss \n",
    "            \n",
    "            if iteration % 10 == 9: # 每 100次记录到 train中\n",
    "                train_writer.add_summary(summary, iteration)              \n",
    "            if iteration % 100 == 0: # 每 1000次输出训练进度结果并保存模型\n",
    "                end = time.time()\n",
    "                summary, acc = sess.run([merged, accuracy], \n",
    "                                        feed_dict={X: X_test, y_: y_test})\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                print(\"Iterations: %d \" % (iteration), \n",
    "                      \"Avg. Train_loss: %.4f \" % (avg_loss / 100), \n",
    "                      \"%.4f sec / batch\" % ((end - start) / 100), \n",
    "                      \"test acc iter %d: %.4f\" % (iteration, acc))\n",
    "                \n",
    "                save_path = saver.save(sess, path + \"models/lstm.ckpt\", global_step=iteration)\n",
    "                avg_loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            iteration += 1\n",
    "train_writer.close()\n",
    "test_writer.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
